{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exploring_huggingface.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "D8nETDBoRMLX",
        "XW9BaK3rdrwL",
        "7ew07kVshL9k"
      ],
      "authorship_tag": "ABX9TyP2rNzzDNmFnaosu3JptYGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "877db406f5774f3b89c4e82caf75c5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_20847ce8389747599ab395c717fd8f48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_35b6046b694843af9fa816d1d8ba0503",
              "IPY_MODEL_cf99e88be27c458d9ca79eec66778768",
              "IPY_MODEL_6470d06f934b4bdf8d8497f2b9df68b4"
            ]
          }
        },
        "20847ce8389747599ab395c717fd8f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35b6046b694843af9fa816d1d8ba0503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_556514f8c87b47a2ad44f03db15cc2f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40fb1a9065474d0eb3970ba360dff44e"
          }
        },
        "cf99e88be27c458d9ca79eec66778768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_89f66fde63824bbba3fe72e7b9f16577",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29bb00fa1ba4424bbf8efd5eadbeb212"
          }
        },
        "6470d06f934b4bdf8d8497f2b9df68b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f8e71aaf83a4a988677ce6e3cebfe42",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00,  9.36it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8d332e0e3c7465d8d9da4286e8e915a"
          }
        },
        "556514f8c87b47a2ad44f03db15cc2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40fb1a9065474d0eb3970ba360dff44e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89f66fde63824bbba3fe72e7b9f16577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29bb00fa1ba4424bbf8efd5eadbeb212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f8e71aaf83a4a988677ce6e3cebfe42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8d332e0e3c7465d8d9da4286e8e915a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeleneFabia/nlp-exploration/blob/main/exploring_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face NLP Course\n",
        "\n"
      ],
      "metadata": {
        "id": "ggj-itvURGAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install libraries\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EL72HWG4Rfk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Datasets)"
      ],
      "metadata": {
        "id": "D8nETDBoRMLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports \n",
        "from datasets import (\n",
        "    load_dataset_builder, \n",
        "    load_dataset,\n",
        ")\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import(\n",
        "    DataLoader\n",
        ")"
      ],
      "metadata": {
        "id": "6lVynTbIRDi5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_builder = load_dataset_builder(path=\"poem_sentiment\")\n",
        "\n",
        "train_dataset = load_dataset(path=\"poem_sentiment\", split=\"train\")\n",
        "# valid_dataset = load_dataset(path=\"poem_sentiment\", split=\"validation\")\n",
        "# test_dataset = load_dataset(path=\"poem_sentiment\", split=\"test\")"
      ],
      "metadata": {
        "id": "q1_6R2RdRabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Description:\", train_dataset.description)\n",
        "print(\"Num data entries:\", len(train_dataset))\n",
        "print(\"Column names:\", train_dataset.column_names)\n",
        "print(\"Classes:\", train_dataset.features[\"label\"].names)\n",
        "print(\"Example data entry:\", train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYEbNPw0ThoP",
        "outputId": "ceb3e084-91ef-488b-e0c6-3e13b6670f76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description: Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg. This dataset can be used for tasks such as sentiment classification or style transfer for poems.\n",
            "\n",
            "Num data entries: 892\n",
            "Column names: ['id', 'verse_text', 'label']\n",
            "Classes: ['negative', 'positive', 'no_impact', 'mixed']\n",
            "Example data entry: {'id': 0, 'verse_text': 'with pale blue berries. in these peaceful shades--', 'label': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "bKBHTgjCUS6X"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc_ds = train_dataset.map(lambda examples: \n",
        "                                          tokenizer(\n",
        "                                              examples[\"verse_text\"], \n",
        "                                              truncation=True,\n",
        "                                              padding=\"max_length\",\n",
        "                                          ),\n",
        "                                 batched=True\n",
        "                                 )"
      ],
      "metadata": {
        "id": "C_szpKZAVZws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Column names of encoded dataset:\", train_enc_ds.column_names)\n",
        "print(\"Tokenized data entry:\", train_enc_ds[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfoaHcQJXmUi",
        "outputId": "53f4e7a5-bfd8-4111-aa3d-2e58e135195b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names of encoded dataset: ['attention_mask', 'id', 'input_ids', 'label', 'token_type_ids', 'verse_text']\n",
            "Tokenized data entry: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 0, 'input_ids': [101, 1114, 4554, 2221, 26571, 119, 1107, 1292, 9441, 16327, 118, 118, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'verse_text': 'with pale blue berries. in these peaceful shades--'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- What are attention mask, input ids and token type ids?"
      ],
      "metadata": {
        "id": "tHiYSk3vYXpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use dataset with pytorch\n",
        "train_enc_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# create pytorch data loader\n",
        "train_dl = DataLoader(train_enc_ds, batch_size=32)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2qNF-9iZmYy",
        "outputId": "aa26e791-b87f-42d0-8cfb-2b77679b5f0b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'input_ids': tensor([[  101,  1114,  4554,  ...,     0,     0,     0],\n",
              "         [  101,  1122,  5611,  ...,     0,     0,     0],\n",
              "         [  101,  1105,  1115,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  1106,  1115,  ...,     0,     0,     0],\n",
              "         [  101,   192,  2386,  ...,     0,     0,     0],\n",
              "         [  101,  1123, 15219,  ...,     0,     0,     0]]),\n",
              " 'label': tensor([1, 2, 0, 3, 3, 3, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 2, 2, 1,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Transformer models"
      ],
      "metadata": {
        "id": "XW9BaK3rdrwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What can Transformers do?\n",
        "\n",
        "Playing around with HuggingFace's OTB models:"
      ],
      "metadata": {
        "id": "la2dcEgcgzkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "CXs80bVxaRsj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"The ocean is beautiful.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Haf-S0XMe5WW",
        "outputId": "3d0b8aa7-0f4a-4c47-9ba0-0863e8b5a906"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998816251754761}]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"Looking at the ocean in front of me, I felt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zyK7aVcfOzF",
        "outputId": "bb988a42-89e6-4395-eb3f-3f382f96b69b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Looking at the ocean in front of me, I felt like an airplane flying right.\\n\\nThe storm had descended very quickly, so it appeared this was about to fall at a later date.\\n\\nWe all started feeling a wave of panic:'}]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"What is my hobby?\",\n",
        "    context=\"I work as an engineer, but in my free time I enjoy cooking.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17-acrlJfVII",
        "outputId": "d113b7d4-42f2-4bde-a58e-777fd8e4ca06"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'cooking', 'end': 58, 'score': 0.428894579410553, 'start': 51}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do Transformers work?"
      ],
      "metadata": {
        "id": "1olfiIiGg1tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important concepts: \n",
        "- **self-supervised learning**: labels are automatically computed from the input\n",
        "- **pretraining**: training a model from scratch on very large amounts of data\n",
        "- **transfer learning**: fine-tuning a pretrained model in a supervised manner with a smaller dataset for a specific language task\n",
        "- **encoder**: receives input and builds representation of it (optimized for  acquiring an understanding from inputs)\n",
        "- **decoder**: receives encoder's representation plus other inputs in order to generate a target sequence (optimized for generating an output)\n",
        "- **encoder-only models** (e.g., BERT, DistilBERT): for tasks that require understanding of the input e.g., sentence classification, named entity recognition\n",
        "- **decoder-only models** (e.g., GPT, GPT-2): for generative tasks e.g., text generation\n",
        "- **encoder-decoder /seq2seq models** (e.g., BART, Marian): for generative tasks that require an input e.g., translation, summarization\n",
        "- **attention layer**: tells the model to pay attention to specific words in the input\n"
      ],
      "metadata": {
        "id": "qXjSScTphGWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Using HuggingFace Transformers"
      ],
      "metadata": {
        "id": "7ew07kVshL9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple pipeline"
      ],
      "metadata": {
        "id": "Bnn_FifDsSyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer:\n",
        "- splits the input words/subwords/symbols (=tokens), since a model cannot process words directly\n",
        "- maps each token to an integer\n",
        "- adds additional inputs necessary for the model\n",
        "- tokenization needs to happen in exactly the same way as was done with the data used for pretraining a model"
      ],
      "metadata": {
        "id": "vPJDLVTxmcly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "\n",
        "from torch.nn.functional import softmax"
      ],
      "metadata": {
        "id": "4gr-W47rgqZ2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "Kd3t9LNtnYh9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = [\"I am looking at the ocean. How beautiful!\"]\n",
        "tokenized_input = tokenizer(input, padding=True, truncation=True)\n",
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSJiXRmynjKI",
        "outputId": "24b1c0f8-7ec4-443e-b8e7-f7473fcfb25e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1045, 2572, 2559, 2012, 1996, 4153, 1012, 2129, 3376, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input[\"input_ids\"] = torch.tensor(tokenized_input[\"input_ids\"])\n",
        "tokenized_input[\"attention_mask\"] = torch.tensor(tokenized_input[\"attention_mask\"])\n",
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUXDuJPAotdr",
        "outputId": "20078822-2f04-4235-82e1-049d1b7fd52c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1045, 2572, 2559, 2012, 1996, 4153, 1012, 2129, 3376,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "input vector:\n",
        "- of shape (batch_size, sequence_length, hidden_size)\n",
        "- batch_size: number of sequences per batch\n",
        "- sequence_length: length of numerical representation of sequence\n",
        "- hidden_size:  vector dimension of each model input (depends on the model)"
      ],
      "metadata": {
        "id": "V8p8kRf1oWSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "U5hIS-vdn0Dx"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**tokenized_input)\n",
        "print(output.logits)\n",
        "prediction = softmax(output.logits, dim=1)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_XgEvwep2zt",
        "outputId": "02600a3e-56b7-4704-be9a-d4d794ce1492"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.3032,  4.5966]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[1.3640e-04, 9.9986e-01]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = 0\n",
        "class_prediction = int(torch.argmax(prediction))\n",
        "print(f\"Prediction for sentiment of '{input[input_id]}':\", \n",
        "      model.config.id2label[class_prediction],\n",
        "      f\"with {prediction.tolist()[input_id][class_prediction]:.4f}% probability\"\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVqVEHzBqaZA",
        "outputId": "5f27c33b-5eb3-40c8-a52a-f2534e083c14"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for sentiment of 'I am looking at the ocean. How beautiful!': POSITIVE with 0.9999% probability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "X3cO9btcsWl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from transformers import (\n",
        "    BertConfig,\n",
        "    BertModel,\n",
        ")"
      ],
      "metadata": {
        "id": "G9x-xh0VsX5y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig()\n",
        "print(config)\n",
        "model = BertModel(config)  # randomly initialized\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")  # pretrained (https://huggingface.co/bert-base-cased)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUIZfUHMsy_H",
        "outputId": "b332b4db-683d-4dc6-dc92-e97361bc8d9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/path/to/my_trained_model\")"
      ],
      "metadata": {
        "id": "f3sT4dr4s25i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizers"
      ],
      "metadata": {
        "id": "OV0-urZ1-R3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used to transform language data into numerical data so that te model can process it. Some approach are:\n",
        "\n",
        "**Word-based tokenizer**\n",
        "- split raw text into words and find numerical representation for them\n",
        "- would need A LOT of different input IDs (one for each word in a language) \n",
        "- no means of showing relationships between words (\"dog\" and \"dogs\" would have different input IDs)\n",
        "- need \"unknown\" token (\"[UNK]\") for words that are not in the vocabulary.\n",
        "\n",
        "**Character-based tokenizer**\n",
        "- raw text is split into characters\n",
        "- fewer distinct input IDs are necessary but numerical sequences would be much longer with this approach\n",
        "\n",
        "**Subword tokenizer**\n",
        "- frequently used words remain as they are, less frequently used ones are split into meaningful subwords (e.g., \"annoyingly\" --> \"annoying\" + \"ly\")\n",
        "- good tradeoff between small number of distinct input IDs and short sequences\n",
        "- examples: WordPiece (BERT), BPE (GPT-2), and Unigram"
      ],
      "metadata": {
        "id": "XK1v2Nv_-a3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    AutoTokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "H2PhghDD-EOE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# essentially the same, but the second module is a wrapper that can be used with any checkpoint"
      ],
      "metadata": {
        "id": "Rdnmd-6e-Ebk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"The sea is incredibly blue and glittering today.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz1CZNn4-EfE",
        "outputId": "2a49f593-1ece-4e39-e7ea-f6290db9363c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1109, 2343, 1110, 12170, 2221, 1105, 22837, 2052, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encoding** = general process of converting text to numbers\n",
        "\n",
        "tokenization = splitting text into tokens (according to the way it was done for the pretrained model we want to use)"
      ],
      "metadata": {
        "id": "34r1EEXWAw2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(\"The sea is incredibly blue and glittering today.\")\n",
        "print(tokens)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZBnVsCw-Eic",
        "outputId": "c287046c-e518-4749-94fe-a6d525ec1cd0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sea', 'is', 'incredibly', 'blue', 'and', 'glittering', 'today', '.']\n",
            "[1109, 2343, 1110, 12170, 2221, 1105, 22837, 2052, 119]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**decoding** = converting numbers to text"
      ],
      "metadata": {
        "id": "CEJaOF7OB4wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN6udvxN-Elx",
        "outputId": "3238ce0a-23cc-4e83-b371-f0362d8167a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sea is incredibly blue and glittering today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling multiple sequences"
      ],
      "metadata": {
        "id": "8Roxdcb8CKUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")"
      ],
      "metadata": {
        "id": "BlnPpCuDCCDD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "_0JM6A4iDFxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"Hmmm, I love green tea!\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input = torch.tensor([ids]) # model expects a batch, not one single sample\n",
        "print(input.shape)\n",
        "\n",
        "output = model(input)\n",
        "print(output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YViKlcN_DZZE",
        "outputId": "a891944e-eeed-478a-e5b6-d12fe8349158"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8])\n",
            "tensor([[-2.6319,  2.8690]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**padding** = making sure all sequences have the same length by adding a padding token"
      ],
      "metadata": {
        "id": "uwrupiBFGF26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"Hmmm, I love green tea!\", \"It's Thursday\"]  # sequences are of different length!\n",
        "try:\n",
        "  inputs = tokenizer(sequences, return_tensors=\"pt\")\n",
        "  input = inputs[\"input_ids\"]\n",
        "  print(input.shape)\n",
        "  output = model(input)\n",
        "  print(output.logits)\n",
        "except ValueError as error:\n",
        "  print(error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdotW-PyEM1n",
        "outputId": "958b1b98-890e-49b2-c028-bdaca5ab45a6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**attention mask** = tensor of exact same shape as input IDs; filled with 0s and 1s; 1 means a specific token is paid attention to in the attention layer, 0 means it is not paid attention to"
      ],
      "metadata": {
        "id": "UI0VAmFgGX9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [[5, 5, 5], [5, 5, tokenizer.pad_token_id]]\n",
        "attention_mask = [[1, 1, 1], [1, 1, 0]]\n",
        "\n",
        "outputs = model(torch.tensor(input_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aowPFPudFSQ7",
        "outputId": "d5c42000-2c5b-4843-bce7-af34c9d71bc0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8322, -0.7892],\n",
            "        [ 0.3235, -0.2539]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**truncation** = limiting the length of a sequence\n",
        "(necessary because models can only handle up to 512/1024 tokens per sequence - however, there are also models (e.g. Longformer) which can handle longer sequences)"
      ],
      "metadata": {
        "id": "kk-GDVtcHr75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    sequences, \n",
        "    padding=True, \n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"  # or \"np\" for numpy arrays\n",
        "    )"
      ],
      "metadata": {
        "id": "p6p0U2xzIQHN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**special tokens** = added to the inputs, for example [CLS] (= beginning of a sequence) and [SEP] (= end of a sequence)"
      ],
      "metadata": {
        "id": "2OF43e2CIDNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[\"input_ids\"][0])\n",
        "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VoAwmDvIFE4",
        "outputId": "d5709ab3-7730-4832-deed-df85163e5547"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  101, 17012,  2213,  1010,  1045,  2293,  2665,  5572,   999,   102])\n",
            "[CLS] hmmm, i love green tea! [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fine-tuning a pretrained model"
      ],
      "metadata": {
        "id": "YI61NOjTLRzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the data"
      ],
      "metadata": {
        "id": "ugOIBcAAMRj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "ODTrxt7dJxVi"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
        "print(\"Columns:\", ds.column_names)\n",
        "print(\"Number of samples:\", len(ds))\n",
        "print(\"Classes:\", ds.features[\"label\"].names)\n",
        "print(\"Example:\", ds[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frnugPTwMzoP",
        "outputId": "814dfcb5-543c-469c-abe6-0c26422967d0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['sentence1', 'sentence2', 'label', 'idx']\n",
            "Number of samples: 3668\n",
            "Classes: ['not_equivalent', 'equivalent']\n",
            "Example: {'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "1LxFinwsNIPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**token type ids** = in this example, the tensor tells the model which input ids belong to the first sentence and which belong to the second sentence."
      ],
      "metadata": {
        "id": "S8v-5NGcOs5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(ds[\"sentence1\"][10], ds[\"sentence2\"][10])\n",
        "print(inputs[\"input_ids\"])\n",
        "print(inputs[\"token_type_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIyWymhdNuwv",
        "outputId": "ea520d16-ee52-4a39-fa39-7ab850c9c956"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 6094, 2437, 2009, 6211, 2005, 10390, 2000, 22505, 2037, 13930, 1999, 10528, 2457, 2180, 10827, 2160, 6226, 1999, 2233, 1012, 102, 6094, 2437, 2009, 6211, 2005, 10390, 2000, 22505, 2037, 13930, 1999, 10528, 2457, 2180, 26203, 1010, 2160, 6226, 1999, 2233, 1998, 2001, 11763, 2011, 1996, 2317, 2160, 1012, 102]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = tokenizer(\n",
        "    ds[\"sentence1\"],\n",
        "    ds[\"sentence1\"],\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")  # will require a lot of RAM; will return the dataset as a dictionary"
      ],
      "metadata": {
        "id": "2amlDhHDOj0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_func(dataset):  # use with Dataset.map() method\n",
        "  return tokenizer(dataset[\"sentence1\"], dataset[\"sentence2\"], truncation=True)\n",
        "  # no padding, since whole dataset would be padded to the same length (unnecessary)\n",
        "  # instead, padding is applied to each batch "
      ],
      "metadata": {
        "id": "OzS6EF2nPqt0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"glue\", \"mrpc\")\n",
        "print(ds)\n",
        "tokenized_ds = ds.map(tokenize_func, batched=True)\n",
        "print(tokenized_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613,
          "referenced_widgets": [
            "877db406f5774f3b89c4e82caf75c5dc",
            "20847ce8389747599ab395c717fd8f48",
            "35b6046b694843af9fa816d1d8ba0503",
            "cf99e88be27c458d9ca79eec66778768",
            "6470d06f934b4bdf8d8497f2b9df68b4",
            "556514f8c87b47a2ad44f03db15cc2f5",
            "40fb1a9065474d0eb3970ba360dff44e",
            "89f66fde63824bbba3fe72e7b9f16577",
            "29bb00fa1ba4424bbf8efd5eadbeb212",
            "5f8e71aaf83a4a988677ce6e3cebfe42",
            "a8d332e0e3c7465d8d9da4286e8e915a"
          ]
        },
        "id": "TD0iDTR6PrA9",
        "outputId": "183cc3d0-8ecd-42dd-9cd0-44780aca846e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "877db406f5774f3b89c4e82caf75c5dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b8e53849ba067d19.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-04e9d6999f15f318.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f66963ee13090126.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dynamic padding** = apply padding in the collate function that builds the DataLoader"
      ],
      "metadata": {
        "id": "zdyjKzBsRseV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_tU8K1JnQp4A"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = tokenized_ds[\"train\"][:6]\n",
        "print([len(sample) for sample in samples[\"input_ids\"]]) # sequences are still of different lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI5jxQF3R681",
        "outputId": "138a73fb-af6f-43f2-eefa-127d9fe6d556"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50, 59, 47, 67, 59, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "batch = data_collator(samples)  # automatically padded to max length in whole dataset\n",
        "print({k: v.shape for k, v in batch.items()})  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMG5sHYwR6_7",
        "outputId": "477f48cb-f399-4edb-f74e-2bbdd4d3842a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': torch.Size([6, 67]), 'input_ids': torch.Size([6, 67]), 'token_type_ids': torch.Size([6, 67]), 'labels': torch.Size([6])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a model with the Trainer API"
      ],
      "metadata": {
        "id": "igR6lRrzTnLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "1RIRVNLNUCC1"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nCoVoxIgUHqm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}