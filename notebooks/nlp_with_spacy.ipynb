{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_with_spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sS-KNnanpYiG",
        "CItLqcFFzfnM"
      ],
      "authorship_tag": "ABX9TyONl6BW8wJutafo1xTCcUQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeleneFabia/nlp-exploration/blob/main/notebooks/nlp_with_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP with SpaCy"
      ],
      "metadata": {
        "id": "-lXsKNAhpU-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "rbgph1DZslaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "O0GEXIsG9k0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Finding words, phrases, names and concepts\n"
      ],
      "metadata": {
        "id": "sS-KNnanpYiG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KuW4x3GhpDvJ"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")  # nlp object"
      ],
      "metadata": {
        "id": "eTCwGCJ_pSJm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hello world!\")\n",
        "print(type(doc), type(doc.text))\n",
        "\n",
        "token = doc[1]\n",
        "print(token.text)\n",
        "\n",
        "span = doc[1:3]\n",
        "print(span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ws9bcOupfYT",
        "outputId": "952d4581-994c-48cb-ecf8-7a6898eadbc6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'> <class 'str'>\n",
            "world\n",
            "world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"It costs €5.\")\n",
        "print([token.i for token in doc])\n",
        "print([token.text for token in doc])\n",
        "print([token.is_alpha for token in doc])\n",
        "print([token.is_punct for token in doc])\n",
        "print([token.like_num for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPsA194GqC8n",
        "outputId": "d5967731-01f0-423d-cbd2-fae55f70459b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4]\n",
            "['It', 'costs', '€', '5', '.']\n",
            "[True, True, False, False, False]\n",
            "[False, False, False, False, True]\n",
            "[False, False, False, True, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\n",
        "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
        "    \"Now less than 4% are.\"\n",
        ")\n",
        "for token in doc:\n",
        "  if token.like_num and doc[token.i + 1].text == \"%\":\n",
        "      print(doc[token.i : token.i + 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z8y-pwFq0e9",
        "outputId": "b0da8db3-ca65-4055-e053-248e6851c756"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60%\n",
            "4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # trained pipeline package \n",
        "doc = nlp(\"She ate the pizza.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{token.text}\\t --> Part-of-speech tag: {token.pos_} \\t| Dependencies: {token.dep_}, {token.head.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MzKKjntr_qM",
        "outputId": "5d522b0e-1afa-41f4-ba16-70f252e94782"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She\t --> Part-of-speech tag: PRON \t| Dependencies: nsubj, ate\n",
            "ate\t --> Part-of-speech tag: VERB \t| Dependencies: ROOT, ate\n",
            "the\t --> Part-of-speech tag: DET \t| Dependencies: det, pizza\n",
            "pizza\t --> Part-of-speech tag: NOUN \t| Dependencies: dobj, ate\n",
            ".\t --> Part-of-speech tag: PUNCT \t| Dependencies: punct, ate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for ent in doc.ents:  # named entities\n",
        "  print(f\"{ent.text} --> {ent.label_} ({spacy.explain(ent.label_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O7iD5U8sy7r",
        "outputId": "7fe3cfb7-8130-4b31-cd4b-0c4c6d4c4363"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple --> ORG (Companies, agencies, institutions, etc.)\n",
            "U.K. --> GPE (Countries, cities, states)\n",
            "$1 billion --> MONEY (Monetary values, including unit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher # rule-based matching (spacy's alternative to regex)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]  # list of dicts (one per token)\n",
        "# \"TEXT\" -- match exact token\n",
        "# \"LOWER\" -- match lexical attributes\n",
        "# \"LEMMA\" -- match lemma (e.g. \"LEMMA\": \"buy\" matches any form of \"buy\")\n",
        "# \"POS\" -- match part-of-speech (e.g. \"POS\": \"NOUN\" matches any noun)\n",
        "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
        "\n",
        "doc = nlp(\"Upcoming iPhone X release date leaked.\")\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "  print(f\"Match ID: {match_id}, \"\n",
        "        f\"Start Idx: {start}, \"\n",
        "        f\"End Idx: {end}\")\n",
        "  matched_span = doc[start:end]\n",
        "  print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4peuMMduDiD",
        "outputId": "13e69d03-2214-4e10-9473-175570c464da"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match ID: 9528407286733565721, Start Idx: 1, End Idx: 3\n",
            "iPhone X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
        "pattern = [\n",
        "           {\"IS_DIGIT\": True},\n",
        "           {\"LOWER\": \"fifa\"},\n",
        "           {\"LOWER\": \"world\"},\n",
        "           {\"LOWER\": \"cup\"},\n",
        "           {\"IS_PUNCT\": True}\n",
        "]\n",
        "matcher.add(\"FIFA\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  print(doc[start: end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl6cSYDEvzL3",
        "outputId": "dbd40947-abfa-4666-c69e-dd1cb85f898d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018 FIFA World Cup:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
        "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}]\n",
        "matcher.add(\"LOVE\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  print(doc[start: end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt7ptGqYyJKH",
        "outputId": "7cbafe82-902c-4638-aad4-0f99b7afe65d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loved dogs\n",
            "love cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
        "pattern = [\n",
        "           {\"LEMMA\": \"buy\"}, \n",
        "           {\"POS\": \"DET\", \"OP\": \"?\"},  # determinant is optional \n",
        "           {\"POS\": \"NOUN\"}\n",
        "           ]\n",
        "# \"OP\": \"!\" -- match 0 times\n",
        "# \"OP\": \"?\" -- match 0-1 times\n",
        "# \"OP\": \"+\" -- match >1 times\n",
        "# \"OP\": \"*\" -- match >0 times \n",
        "matcher.add(\"BUY\", [pattern])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  print(doc[start: end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHQP3UvXyo-B",
        "outputId": "00a11813-6dc8-4da3-be54-c30bc3a2fbb6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bought a smartphone\n",
            "buying apps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Large-scale data analysis"
      ],
      "metadata": {
        "id": "CItLqcFFzfnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab stores data across multiple documents, strings are saved as hash values \n",
        "\n",
        "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
        "coffee_string = nlp.vocab.strings[coffee_hash]\n",
        "\n",
        "print(coffee_hash, coffee_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A2xS82dzCHQ",
        "outputId": "5efc6f33-d1d9-4c39-e367-37911c89c954"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3197928453018144401 coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I love tea.\")\n",
        "nlp.vocab.strings.add(\"coffee\")\n",
        "\n",
        "tea_hash = nlp.vocab.strings[\"tea\"]\n",
        "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
        "\n",
        "print(nlp.vocab.strings[\"tea\"], nlp.vocab.strings[tea_hash])\n",
        "print(nlp.vocab.strings[\"coffee\"], nlp.vocab.strings[coffee_hash])\n",
        "print(doc.vocab.strings[\"tea\"], doc.vocab.strings[tea_hash])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbXBWvEj2gtl",
        "outputId": "0da4aa3f-1d60-42ba-dc5b-035532e68b66"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6041671307218480733 tea\n",
            "3197928453018144401 coffee\n",
            "6041671307218480733 tea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lexeme = nlp.vocab[\"tea\"]  # lexeme object = entry in vocab\n",
        "print(\n",
        "    lexeme.text, # string\n",
        "    lexeme.orth, # hash value\n",
        "    lexeme.is_alpha\n",
        "    )\n",
        "\n",
        "# doc consists of tokens\n",
        "# each token is stored as lexeme (represented by hash value) in vocab\n",
        "# hash value can be used to obtain string value again"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pDxw8kC3c45",
        "outputId": "1d8f8661-b470-4665-d106-a50e5957e22f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tea 6041671307218480733 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]  # whether space comes after word\n",
        "\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)\n",
        "\n",
        "span = Span(doc, 0, 2)\n",
        "print(span.text)\n",
        "\n",
        "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
        "print(span_with_label.text, span_with_label.label_)\n",
        "\n",
        "doc.ents = [span_with_label]  # add span to doc's entities\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKup9bAg3uyo",
        "outputId": "50590a0b-eaa9-465e-8f29-c6da6647bd9a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n",
            "Hello world\n",
            "Hello world GREETING\n",
            "[('Hello world', 'GREETING')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Berlin looks like a nice city\")\n",
        "\n",
        "# Get all tokens and part-of-speech tags\n",
        "token_texts = [token.text for token in doc]\n",
        "pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "for token in doc:\n",
        "  if token.pos_ == \"PROPN\":\n",
        "    if doc[token.i + 1].pos_ == \"VERB\":\n",
        "      print(\"Found proper noun before a verb:\", doc[token.i].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQFVln8t5GOk",
        "outputId": "2bb94087-134c-4087-f2d4-b609c4f4e4d1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found proper noun before a verb: Berlin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "metadata": {
        "id": "dbymMN8q5SZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like fast food.\")\n",
        "doc2 = nlp(\"I like pizza.\")\n",
        "print(doc1.similarity(doc2))\n",
        "\n",
        "doc = nlp(\"I like pizza and pasta\")\n",
        "token1 = doc[2]\n",
        "token2 = doc[4]\n",
        "print(token1.similarity(token2))\n",
        "\n",
        "# similarity is determined via word vectors (= multi-dimensional representation of words)\n",
        "# word vectors are created via e.g. Word2Vec algorithms and lots of text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXk61TrC81T6",
        "outputId": "3b2e3ca9-7874-43b5-ec2c-11daf008e592"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9009145331610278\n",
            "0.7369546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I have a banana.\")\n",
        "print(\"Shape of word vector for 'banana':\", doc[3].vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBFOXhM1_DlI",
        "outputId": "3bef1249-4053-4981-f871-6ec1db9ac2e6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of word vector for 'banana': (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like cats\")\n",
        "doc2 = nlp(\"I hate cats\")\n",
        "\n",
        "print(doc1.similarity(doc2))\n",
        "\n",
        "# concept of similarity depends!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMKuQ_Ku_Xap",
        "outputId": "035d0b00-8549-44bf-b16d-8d8714062b99"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9501447503553421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\n",
        "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
        "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
        "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
        "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
        "    \"Prime for new members, beginning on September 14. However, members with \"\n",
        "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
        "    \"viewing until their subscription comes up for renewal. Those with \"\n",
        "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
        ")\n",
        "pattern1 = [{\"LOWER\": \"amazon\"}, {\"POS\": \"PROPN\", \"IS_TITLE\": True}]\n",
        "pattern2 = [{\"LOWER\": \"ad\"}, {\"LOWER\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PATTERN1\", [pattern1])\n",
        "matcher.add(\"PATTERN2\", [pattern2])\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "  print(doc.vocab.strings[match_id], doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bTe-lcEAeX9",
        "outputId": "765db870-b9da-4790-ae3e-8e5dfbee64de"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PATTERN1 Amazon Prime\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN1 Amazon Prime\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN2 ad-free viewing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"Italy and Spain are popular tourist destinations in Europe.\")\n",
        "\n",
        "COUNTRIES = [\"Austria\", \"Australia\", \"Belgium\", \"Italy\", \"Spain\", \"Thailand\"]\n",
        "COUNTRIES = nlp.pipe(COUNTRIES)\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "matcher.add(\"COUNTRY\", COUNTRIES)\n",
        "print([doc[start:end] for match_id, start, end in matcher(doc)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FltNuBHBZ80",
        "outputId": "847c5aff-91e4-4afb-fbdf-98817ac63b04"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Italy, Spain]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Processing Pipelines"
      ],
      "metadata": {
        "id": "aUqUhqDzEAEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lOcEfDztDfuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}