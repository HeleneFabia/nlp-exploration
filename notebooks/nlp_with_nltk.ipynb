{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_with_nltk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqz4xud9W+GhTqAH/nQi32",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeleneFabia/nlp-exploration/blob/main/notebooks/nlp_with_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP with NLTK"
      ],
      "metadata": {
        "id": "im653Tqtb0TT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dCi8OXaSI38h"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "8ywOFhfxb41j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "dXWPr4dBbzOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting a text into sentences and words\n",
        "example_string = (\n",
        "    \"Societies construct patterns of behavior by deeming certain \" \n",
        "    \"actions or concepts as acceptable or unacceptable. These patterns of \" \n",
        "    \"behavior within a given society are known as societal norms. Societies, \"\n",
        "    \"and their norms, undergo gradual and perpetual changes.\"\n",
        "    )\n",
        "\n",
        "sentences = sent_tokenize(example_string)\n",
        "words = word_tokenize(sentences[0])\n",
        "print(sentences)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOVShhYXdYJu",
        "outputId": "ab53a378-36b0-41e7-c8db-b594b78459a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Societies construct patterns of behavior by deeming certain actions or concepts as acceptable or unacceptable.', 'These patterns of behavior within a given society are known as societal norms.', 'Societies, and their norms, undergo gradual and perpetual changes.']\n",
            "['Societies', 'construct', 'patterns', 'of', 'behavior', 'by', 'deeming', 'certain', 'actions', 'or', 'concepts', 'as', 'acceptable', 'or', 'unacceptable', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering Stop Words"
      ],
      "metadata": {
        "id": "bL8sr0t_hNWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "9nQClZyId3Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I'm a happy person, at least most of the time.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = [word for word in words if word.casefold() not in stop_words]\n",
        "# .casefold() to make it case-insensitive\n",
        "print(filtered_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRbfQnuKe_Cf",
        "outputId": "496ef0bf-33f9-4b62-be79-62563cd7d91c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'a', 'happy', 'person', ',', 'at', 'least', 'most', 'of', 'the', 'time', '.']\n",
            "[\"'m\", 'happy', 'person', ',', 'least', 'time', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "(= reducing a word to its root)"
      ],
      "metadata": {
        "id": "9iJCxsDShQ22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "words = word_tokenize(sentences[0])\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Unstemmed:\", words)\n",
        "print(\"Stemmed:\", stemmed_words)\n",
        "\n",
        "# understemming: when two related words should be reduced to the same stem but are not (FN)\n",
        "# overstemming: when two unrelated words are reduced to the same stem but should not be (FP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzHjOubNfVQK",
        "outputId": "d51742b8-43ba-4831-8fdf-ec81cdedeeb4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstemmed: ['Societies', 'construct', 'patterns', 'of', 'behavior', 'by', 'deeming', 'certain', 'actions', 'or', 'concepts', 'as', 'acceptable', 'or', 'unacceptable', '.']\n",
            "Stemmed: ['societi', 'construct', 'pattern', 'of', 'behavior', 'by', 'deem', 'certain', 'action', 'or', 'concept', 'as', 'accept', 'or', 'unaccept', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts of Speech (POS) tagging \n",
        "(noun, pronoun, adjective, verb, adverb, preposition, conjunction, interjection, determiner)"
      ],
      "metadata": {
        "id": "YdFaCro5hZaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "id": "QM1PL8-KhwCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w, pos in nltk.pos_tag(words):\n",
        "  print(f\"{w} --- {pos}\")\n",
        "\n",
        "# nltk.help.upenn_tagset() for POS descriptions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC5cEhDegMNt",
        "outputId": "6086d28b-96ac-4d82-fa84-823167fdaed5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Societies --- NNS\n",
            "construct --- VBP\n",
            "patterns --- NNS\n",
            "of --- IN\n",
            "behavior --- NN\n",
            "by --- IN\n",
            "deeming --- VBG\n",
            "certain --- JJ\n",
            "actions --- NNS\n",
            "or --- CC\n",
            "concepts --- NNS\n",
            "as --- IN\n",
            "acceptable --- JJ\n",
            "or --- CC\n",
            "unacceptable --- JJ\n",
            ". --- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizing\n",
        "(= reduce word to its core meaning, but returns an actual word rather than only a fragment when stemming)\n",
        "\n",
        "lemma = word that represents a whole group of words (= lexeme)\n",
        "\n",
        "e.g. \"swimming\" --> lemma is \"swim\", \"swimming\" is part of the lexeme"
      ],
      "metadata": {
        "id": "CPXu9Ds0iim0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "XED16S8xjWeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"beautifully\"))\n",
        "print(lemmatizer.lemmatize(\"boats\"))\n",
        "print(lemmatizer.lemmatize(\"loved\"))\n",
        "print(lemmatizer.lemmatize(\"loved\", pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"worst\"))\n",
        "print(lemmatizer.lemmatize(\"worst\", pos=\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY5fcJylhtAh",
        "outputId": "c0e99372-b3fb-4e25-f250-fbbad997735a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beautifully\n",
            "boat\n",
            "loved\n",
            "love\n",
            "worst\n",
            "bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking\n",
        "(= identify phrases)\n",
        "\n",
        "uses POS tags to group and chunk words"
      ],
      "metadata": {
        "id": "56QYq3fmkouo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quote = \"It's a dangerous business, Frodo, going out your door.\"\n",
        "words = nltk.word_tokenize(quote)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # rule combination for how sentences should be chunked\n",
        "# chunk noun phrases (NP)\n",
        "# start with optional (?) determiner (DT)\n",
        "# have any number (*) of adjectives (JJ)\n",
        "# end with a noun (NN)\n",
        "\n",
        "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
        "tree = chunk_parser.parse(pos_tags)"
      ],
      "metadata": {
        "id": "6pla0d-Uj89l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UImsevTCk81o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}