{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_with_huggingface.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XW9BaK3rdrwL",
        "7ew07kVshL9k",
        "YI61NOjTLRzp",
        "D8nETDBoRMLX",
        "z-a0M-C9bcwB",
        "eXQi92am52Ef"
      ],
      "authorship_tag": "ABX9TyNBIbE0+zsHiXLyZGxXTckx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2366d78eee34564b5ddb81ef1e3f9df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_944c636a4ed74bd594ee590b9f4d9767",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aadee3faa2324a398330421f3c791d6b",
              "IPY_MODEL_772c71e15ed843a3a2c128f4a6a4f993",
              "IPY_MODEL_f2472e3fb5d748849bba736d1e878cbc"
            ]
          }
        },
        "944c636a4ed74bd594ee590b9f4d9767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aadee3faa2324a398330421f3c791d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bdf4970dd87247b1a38f4f14b2bef2cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76f756b9d3ea4afbb40ce402ea0aa2f5"
          }
        },
        "772c71e15ed843a3a2c128f4a6a4f993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a458e0e8a5a4f259e51ad62697f02ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1815442e0ea54d3f82ff5dca53d76ac1"
          }
        },
        "f2472e3fb5d748849bba736d1e878cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c65dc5ac07f74ec68e1aa64a3d067764",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 57.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59f30938374944f1a5f2a258c59d9412"
          }
        },
        "bdf4970dd87247b1a38f4f14b2bef2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76f756b9d3ea4afbb40ce402ea0aa2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a458e0e8a5a4f259e51ad62697f02ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1815442e0ea54d3f82ff5dca53d76ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c65dc5ac07f74ec68e1aa64a3d067764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59f30938374944f1a5f2a258c59d9412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86638f4637694954ba4c47ea7fa657c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e6b1274a82045f29b3b03488dc5a250",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_67d05f6270734c13b91af073076faf47",
              "IPY_MODEL_28b47bb248c94c2384fa4fbded5882ec",
              "IPY_MODEL_6d205bf89cbc4f3fa6afdfe4dcd006c6"
            ]
          }
        },
        "2e6b1274a82045f29b3b03488dc5a250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67d05f6270734c13b91af073076faf47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00b754ceec9a409198adc96e5df4b201",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c80ac0e8f1384342a0e9fa6000a1ca16"
          }
        },
        "28b47bb248c94c2384fa4fbded5882ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e43e199ba94f415ca2f774590c405c20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_914ce147edbe4c308b0ffa2dd35ed851"
          }
        },
        "6d205bf89cbc4f3fa6afdfe4dcd006c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cff1bb1518e544ed932264857c4f6903",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00,  6.09ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_667144992b8d45b8918a6ae220725561"
          }
        },
        "00b754ceec9a409198adc96e5df4b201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c80ac0e8f1384342a0e9fa6000a1ca16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e43e199ba94f415ca2f774590c405c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "914ce147edbe4c308b0ffa2dd35ed851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cff1bb1518e544ed932264857c4f6903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "667144992b8d45b8918a6ae220725561": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8a96cb943de4cf39f2b6a9876e55141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_253f3722292247488e5eacc1efe8ad46",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b08ef877b51f420492d7b3569e72d31b",
              "IPY_MODEL_049c2becd460423583087dfaebbd4e9d",
              "IPY_MODEL_4bb229c9625348f6b98b140b84efa1e0"
            ]
          }
        },
        "253f3722292247488e5eacc1efe8ad46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b08ef877b51f420492d7b3569e72d31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f86476e1c4a74f17a7cc80d86668a647",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2cd763587be54a868c1947730d82ee32"
          }
        },
        "049c2becd460423583087dfaebbd4e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d155ba5846b74184ad81cf7a591203dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e56cb107bf34af1a60393f4dbd81257"
          }
        },
        "4bb229c9625348f6b98b140b84efa1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22829a9cd4f84f34b87281df5c66bb2a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  7.07ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd96b11555b84777a397675495f90871"
          }
        },
        "f86476e1c4a74f17a7cc80d86668a647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2cd763587be54a868c1947730d82ee32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d155ba5846b74184ad81cf7a591203dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e56cb107bf34af1a60393f4dbd81257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22829a9cd4f84f34b87281df5c66bb2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd96b11555b84777a397675495f90871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d329ec16e174052ae7b2ae195055c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b9eb064e07a14575913afea9774cf43b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6bb7aac74ea482dbd6de7a019f63a0a",
              "IPY_MODEL_ba696fc8349644b8ba72a4dd28ef531b",
              "IPY_MODEL_b88a56c26db54ba6beebaf7017478b02"
            ]
          }
        },
        "b9eb064e07a14575913afea9774cf43b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6bb7aac74ea482dbd6de7a019f63a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0036c351554c4006961c8b44805a6158",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b5ed87c79a24c0d8d4b31ad708d6a82"
          }
        },
        "ba696fc8349644b8ba72a4dd28ef531b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0f7af2510b8a4987af9519d11f0b582e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa745fa819d04d199f75ad7578d80aea"
          }
        },
        "b88a56c26db54ba6beebaf7017478b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7449205bb3b24f2bb7c5b49d35478461",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00,  6.02ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4753373ccdb246b7b246242eca35649e"
          }
        },
        "0036c351554c4006961c8b44805a6158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b5ed87c79a24c0d8d4b31ad708d6a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f7af2510b8a4987af9519d11f0b582e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa745fa819d04d199f75ad7578d80aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7449205bb3b24f2bb7c5b49d35478461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4753373ccdb246b7b246242eca35649e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ae89c11749a4ae7af5d7dd80f0a1ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce226a3b1b914a25b432d8d0240baabb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba03293bd5fa4d38888947fc764f1d7b",
              "IPY_MODEL_21420de77ce54c169139201b99ceac34",
              "IPY_MODEL_385c5f3be32546c8abe5faa308a4c33c"
            ]
          }
        },
        "ce226a3b1b914a25b432d8d0240baabb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba03293bd5fa4d38888947fc764f1d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a35e1f0e5b3d4348bf96a866b3be5d79",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_26a6233e6f9246a7be33599fe34f608c"
          }
        },
        "21420de77ce54c169139201b99ceac34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0f8d6ee8a0af4f55aa19dcfde12360ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20d9a2a3766a4d07ad746dd560eca686"
          }
        },
        "385c5f3be32546c8abe5faa308a4c33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_efe120f19cdf4b23b93ffe1a557f11ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 420M/420M [00:13&lt;00:00, 34.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f29d04ae86b049328e3f378fa8bdfbd4"
          }
        },
        "a35e1f0e5b3d4348bf96a866b3be5d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "26a6233e6f9246a7be33599fe34f608c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f8d6ee8a0af4f55aa19dcfde12360ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20d9a2a3766a4d07ad746dd560eca686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efe120f19cdf4b23b93ffe1a557f11ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f29d04ae86b049328e3f378fa8bdfbd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeleneFabia/nlp-exploration/blob/main/notebooks/nlp_with_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face NLP Course\n",
        "\n"
      ],
      "metadata": {
        "id": "ggj-itvURGAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install libraries\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "EL72HWG4Rfk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Transformer models"
      ],
      "metadata": {
        "id": "XW9BaK3rdrwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What can Transformers do?\n",
        "\n",
        "Playing around with HuggingFace's OTB models:"
      ],
      "metadata": {
        "id": "la2dcEgcgzkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "CXs80bVxaRsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"The ocean is beautiful.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Haf-S0XMe5WW",
        "outputId": "3d0b8aa7-0f4a-4c47-9ba0-0863e8b5a906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998816251754761}]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"Looking at the ocean in front of me, I felt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zyK7aVcfOzF",
        "outputId": "bb988a42-89e6-4395-eb3f-3f382f96b69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Looking at the ocean in front of me, I felt like an airplane flying right.\\n\\nThe storm had descended very quickly, so it appeared this was about to fall at a later date.\\n\\nWe all started feeling a wave of panic:'}]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"What is my hobby?\",\n",
        "    context=\"I work as an engineer, but in my free time I enjoy cooking.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17-acrlJfVII",
        "outputId": "d113b7d4-42f2-4bde-a58e-777fd8e4ca06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'cooking', 'end': 58, 'score': 0.428894579410553, 'start': 51}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do Transformers work?"
      ],
      "metadata": {
        "id": "1olfiIiGg1tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important concepts: \n",
        "- **self-supervised learning**: labels are automatically computed from the input\n",
        "- **pretraining**: training a model from scratch on very large amounts of data\n",
        "- **transfer learning**: fine-tuning a pretrained model in a supervised manner with a smaller dataset for a specific language task\n",
        "- **encoder**: receives input and builds representation of it (optimized for  acquiring an understanding from inputs)\n",
        "- **decoder**: receives encoder's representation plus other inputs in order to generate a target sequence (optimized for generating an output)\n",
        "- **encoder-only models** (e.g., BERT, DistilBERT): for tasks that require understanding of the input e.g., sentence classification, named entity recognition\n",
        "- **decoder-only models** (e.g., GPT, GPT-2): for generative tasks e.g., text generation\n",
        "- **encoder-decoder /seq2seq models** (e.g., BART, Marian): for generative tasks that require an input e.g., translation, summarization\n",
        "- **attention layer**: tells the model to pay attention to specific words in the input\n"
      ],
      "metadata": {
        "id": "qXjSScTphGWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Using HuggingFace Transformers"
      ],
      "metadata": {
        "id": "7ew07kVshL9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple pipeline"
      ],
      "metadata": {
        "id": "Bnn_FifDsSyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer:\n",
        "- splits the input words/subwords/symbols (=tokens), since a model cannot process words directly\n",
        "- maps each token to an integer\n",
        "- adds additional inputs necessary for the model\n",
        "- tokenization needs to happen in exactly the same way as was done with the data used for pretraining a model"
      ],
      "metadata": {
        "id": "vPJDLVTxmcly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "\n",
        "from torch.nn.functional import softmax"
      ],
      "metadata": {
        "id": "4gr-W47rgqZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "Kd3t9LNtnYh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = [\"I am looking at the ocean. How beautiful!\"]\n",
        "tokenized_input = tokenizer(input, padding=True, truncation=True)\n",
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSJiXRmynjKI",
        "outputId": "24b1c0f8-7ec4-443e-b8e7-f7473fcfb25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1045, 2572, 2559, 2012, 1996, 4153, 1012, 2129, 3376, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input[\"input_ids\"] = torch.tensor(tokenized_input[\"input_ids\"])\n",
        "tokenized_input[\"attention_mask\"] = torch.tensor(tokenized_input[\"attention_mask\"])\n",
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUXDuJPAotdr",
        "outputId": "20078822-2f04-4235-82e1-049d1b7fd52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1045, 2572, 2559, 2012, 1996, 4153, 1012, 2129, 3376,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "input vector:\n",
        "- of shape (batch_size, sequence_length, hidden_size)\n",
        "- batch_size: number of sequences per batch\n",
        "- sequence_length: length of numerical representation of sequence\n",
        "- hidden_size:  vector dimension of each model input (depends on the model)"
      ],
      "metadata": {
        "id": "V8p8kRf1oWSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "U5hIS-vdn0Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**tokenized_input)\n",
        "print(output.logits)\n",
        "prediction = softmax(output.logits, dim=1)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_XgEvwep2zt",
        "outputId": "02600a3e-56b7-4704-be9a-d4d794ce1492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.3032,  4.5966]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[1.3640e-04, 9.9986e-01]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = 0\n",
        "class_prediction = int(torch.argmax(prediction))\n",
        "print(f\"Prediction for sentiment of '{input[input_id]}':\", \n",
        "      model.config.id2label[class_prediction],\n",
        "      f\"with {prediction.tolist()[input_id][class_prediction]:.4f}% probability\"\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVqVEHzBqaZA",
        "outputId": "5f27c33b-5eb3-40c8-a52a-f2534e083c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for sentiment of 'I am looking at the ocean. How beautiful!': POSITIVE with 0.9999% probability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "X3cO9btcsWl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from transformers import (\n",
        "    BertConfig,\n",
        "    BertModel,\n",
        ")"
      ],
      "metadata": {
        "id": "G9x-xh0VsX5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig()\n",
        "print(config)\n",
        "model = BertModel(config)  # randomly initialized\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")  # pretrained (https://huggingface.co/bert-base-cased)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUIZfUHMsy_H",
        "outputId": "b332b4db-683d-4dc6-dc92-e97361bc8d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/path/to/my_trained_model\")"
      ],
      "metadata": {
        "id": "f3sT4dr4s25i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizers"
      ],
      "metadata": {
        "id": "OV0-urZ1-R3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used to transform language data into numerical data so that te model can process it. Some approach are:\n",
        "\n",
        "**Word-based tokenizer**\n",
        "- split raw text into words and find numerical representation for them\n",
        "- would need A LOT of different input IDs (one for each word in a language) \n",
        "- no means of showing relationships between words (\"dog\" and \"dogs\" would have different input IDs)\n",
        "- need \"unknown\" token (\"[UNK]\") for words that are not in the vocabulary.\n",
        "\n",
        "**Character-based tokenizer**\n",
        "- raw text is split into characters\n",
        "- fewer distinct input IDs are necessary but numerical sequences would be much longer with this approach\n",
        "\n",
        "**Subword tokenizer**\n",
        "- frequently used words remain as they are, less frequently used ones are split into meaningful subwords (e.g., \"annoyingly\" --> \"annoying\" + \"ly\")\n",
        "- good tradeoff between small number of distinct input IDs and short sequences\n",
        "- examples: WordPiece (BERT), BPE (GPT-2), and Unigram"
      ],
      "metadata": {
        "id": "XK1v2Nv_-a3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    AutoTokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "H2PhghDD-EOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# essentially the same, but the second module is a wrapper that can be used with any checkpoint"
      ],
      "metadata": {
        "id": "Rdnmd-6e-Ebk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"The sea is incredibly blue and glittering today.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz1CZNn4-EfE",
        "outputId": "2a49f593-1ece-4e39-e7ea-f6290db9363c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1109, 2343, 1110, 12170, 2221, 1105, 22837, 2052, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encoding** = general process of converting text to numbers\n",
        "\n",
        "tokenization = splitting text into tokens (according to the way it was done for the pretrained model we want to use)"
      ],
      "metadata": {
        "id": "34r1EEXWAw2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(\"The sea is incredibly blue and glittering today.\")\n",
        "print(tokens)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZBnVsCw-Eic",
        "outputId": "c287046c-e518-4749-94fe-a6d525ec1cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sea', 'is', 'incredibly', 'blue', 'and', 'glittering', 'today', '.']\n",
            "[1109, 2343, 1110, 12170, 2221, 1105, 22837, 2052, 119]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**decoding** = converting numbers to text"
      ],
      "metadata": {
        "id": "CEJaOF7OB4wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN6udvxN-Elx",
        "outputId": "3238ce0a-23cc-4e83-b371-f0362d8167a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sea is incredibly blue and glittering today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling multiple sequences"
      ],
      "metadata": {
        "id": "8Roxdcb8CKUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")"
      ],
      "metadata": {
        "id": "BlnPpCuDCCDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "_0JM6A4iDFxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"Hmmm, I love green tea!\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input = torch.tensor([ids]) # model expects a batch, not one single sample\n",
        "print(input.shape)\n",
        "\n",
        "output = model(input)\n",
        "print(output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YViKlcN_DZZE",
        "outputId": "a891944e-eeed-478a-e5b6-d12fe8349158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8])\n",
            "tensor([[-2.6319,  2.8690]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**padding** = making sure all sequences have the same length by adding a padding token"
      ],
      "metadata": {
        "id": "uwrupiBFGF26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"Hmmm, I love green tea!\", \"It's Thursday\"]  # sequences are of different length!\n",
        "try:\n",
        "  inputs = tokenizer(sequences, return_tensors=\"pt\")\n",
        "  input = inputs[\"input_ids\"]\n",
        "  print(input.shape)\n",
        "  output = model(input)\n",
        "  print(output.logits)\n",
        "except ValueError as error:\n",
        "  print(error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdotW-PyEM1n",
        "outputId": "958b1b98-890e-49b2-c028-bdaca5ab45a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**attention mask** = tensor of exact same shape as input IDs; filled with 0s and 1s; 1 means a specific token is paid attention to in the attention layer, 0 means it is not paid attention to"
      ],
      "metadata": {
        "id": "UI0VAmFgGX9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [[5, 5, 5], [5, 5, tokenizer.pad_token_id]]\n",
        "attention_mask = [[1, 1, 1], [1, 1, 0]]\n",
        "\n",
        "outputs = model(torch.tensor(input_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aowPFPudFSQ7",
        "outputId": "d5c42000-2c5b-4843-bce7-af34c9d71bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8322, -0.7892],\n",
            "        [ 0.3235, -0.2539]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**truncation** = limiting the length of a sequence\n",
        "(necessary because models can only handle up to 512/1024 tokens per sequence - however, there are also models (e.g. Longformer) which can handle longer sequences)"
      ],
      "metadata": {
        "id": "kk-GDVtcHr75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    sequences, \n",
        "    padding=True, \n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"  # or \"np\" for numpy arrays\n",
        "    )"
      ],
      "metadata": {
        "id": "p6p0U2xzIQHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**special tokens** = added to the inputs, for example [CLS] (= beginning of a sequence) and [SEP] (= end of a sequence)"
      ],
      "metadata": {
        "id": "2OF43e2CIDNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[\"input_ids\"][0])\n",
        "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VoAwmDvIFE4",
        "outputId": "d5709ab3-7730-4832-deed-df85163e5547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  101, 17012,  2213,  1010,  1045,  2293,  2665,  5572,   999,   102])\n",
            "[CLS] hmmm, i love green tea! [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fine-tuning a pretrained model"
      ],
      "metadata": {
        "id": "YI61NOjTLRzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the data"
      ],
      "metadata": {
        "id": "ugOIBcAAMRj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "ODTrxt7dJxVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
        "print(\"Columns:\", ds.column_names)\n",
        "print(\"Number of samples:\", len(ds))\n",
        "print(\"Classes:\", ds.features[\"label\"].names)\n",
        "print(\"Example:\", ds[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frnugPTwMzoP",
        "outputId": "98051ec2-912d-40ce-feb1-733c5d344e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['sentence1', 'sentence2', 'label', 'idx']\n",
            "Number of samples: 3668\n",
            "Classes: ['not_equivalent', 'equivalent']\n",
            "Example: {'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "1LxFinwsNIPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**token type ids** = in this example, the tensor tells the model which input ids belong to the first sentence and which belong to the second sentence."
      ],
      "metadata": {
        "id": "S8v-5NGcOs5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(ds[\"sentence1\"][10], ds[\"sentence2\"][10])\n",
        "print(inputs[\"input_ids\"])\n",
        "print(inputs[\"token_type_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIyWymhdNuwv",
        "outputId": "f7294724-eda9-40c4-979d-21ee36301f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 6094, 2437, 2009, 6211, 2005, 10390, 2000, 22505, 2037, 13930, 1999, 10528, 2457, 2180, 10827, 2160, 6226, 1999, 2233, 1012, 102, 6094, 2437, 2009, 6211, 2005, 10390, 2000, 22505, 2037, 13930, 1999, 10528, 2457, 2180, 26203, 1010, 2160, 6226, 1999, 2233, 1998, 2001, 11763, 2011, 1996, 2317, 2160, 1012, 102]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = tokenizer(\n",
        "    ds[\"sentence1\"],\n",
        "    ds[\"sentence1\"],\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")  # will require a lot of RAM; will return the dataset as a dictionary"
      ],
      "metadata": {
        "id": "2amlDhHDOj0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_func(dataset):  # use with Dataset.map() method\n",
        "  return tokenizer(dataset[\"sentence1\"], dataset[\"sentence2\"], truncation=True)\n",
        "  # no padding, since whole dataset would be padded to the same length (unnecessary)\n",
        "  # instead, padding is applied to each batch "
      ],
      "metadata": {
        "id": "OzS6EF2nPqt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"glue\", \"mrpc\")\n",
        "print(ds)\n",
        "tokenized_ds = ds.map(tokenize_func, batched=True)\n",
        "print(tokenized_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882,
          "referenced_widgets": [
            "c2366d78eee34564b5ddb81ef1e3f9df",
            "944c636a4ed74bd594ee590b9f4d9767",
            "aadee3faa2324a398330421f3c791d6b",
            "772c71e15ed843a3a2c128f4a6a4f993",
            "f2472e3fb5d748849bba736d1e878cbc",
            "bdf4970dd87247b1a38f4f14b2bef2cf",
            "76f756b9d3ea4afbb40ce402ea0aa2f5",
            "9a458e0e8a5a4f259e51ad62697f02ce",
            "1815442e0ea54d3f82ff5dca53d76ac1",
            "c65dc5ac07f74ec68e1aa64a3d067764",
            "59f30938374944f1a5f2a258c59d9412",
            "86638f4637694954ba4c47ea7fa657c0",
            "2e6b1274a82045f29b3b03488dc5a250",
            "67d05f6270734c13b91af073076faf47",
            "28b47bb248c94c2384fa4fbded5882ec",
            "6d205bf89cbc4f3fa6afdfe4dcd006c6",
            "00b754ceec9a409198adc96e5df4b201",
            "c80ac0e8f1384342a0e9fa6000a1ca16",
            "e43e199ba94f415ca2f774590c405c20",
            "914ce147edbe4c308b0ffa2dd35ed851",
            "cff1bb1518e544ed932264857c4f6903",
            "667144992b8d45b8918a6ae220725561",
            "a8a96cb943de4cf39f2b6a9876e55141",
            "253f3722292247488e5eacc1efe8ad46",
            "b08ef877b51f420492d7b3569e72d31b",
            "049c2becd460423583087dfaebbd4e9d",
            "4bb229c9625348f6b98b140b84efa1e0",
            "f86476e1c4a74f17a7cc80d86668a647",
            "2cd763587be54a868c1947730d82ee32",
            "d155ba5846b74184ad81cf7a591203dc",
            "3e56cb107bf34af1a60393f4dbd81257",
            "22829a9cd4f84f34b87281df5c66bb2a",
            "dd96b11555b84777a397675495f90871",
            "4d329ec16e174052ae7b2ae195055c77",
            "b9eb064e07a14575913afea9774cf43b",
            "d6bb7aac74ea482dbd6de7a019f63a0a",
            "ba696fc8349644b8ba72a4dd28ef531b",
            "b88a56c26db54ba6beebaf7017478b02",
            "0036c351554c4006961c8b44805a6158",
            "0b5ed87c79a24c0d8d4b31ad708d6a82",
            "0f7af2510b8a4987af9519d11f0b582e",
            "fa745fa819d04d199f75ad7578d80aea",
            "7449205bb3b24f2bb7c5b49d35478461",
            "4753373ccdb246b7b246242eca35649e"
          ]
        },
        "id": "TD0iDTR6PrA9",
        "outputId": "ff02576b-ca10-4ff3-c2ee-d0988fcea56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2366d78eee34564b5ddb81ef1e3f9df",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86638f4637694954ba4c47ea7fa657c0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8a96cb943de4cf39f2b6a9876e55141",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d329ec16e174052ae7b2ae195055c77",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dynamic padding** = apply padding in the collate function that builds the DataLoader"
      ],
      "metadata": {
        "id": "zdyjKzBsRseV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_tU8K1JnQp4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = tokenized_ds[\"train\"][:6]\n",
        "print([len(sample) for sample in samples[\"input_ids\"]]) # sequences are still of different lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI5jxQF3R681",
        "outputId": "31fdec28-db40-4934-f05a-741edf0cb85d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50, 59, 47, 67, 59, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "batch = data_collator(samples)  # automatically padded to max length in whole dataset\n",
        "print({k: v.shape for k, v in batch.items()})  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMG5sHYwR6_7",
        "outputId": "23cf022f-4bde-45d8-9d17-851627f29078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': torch.Size([6, 67]), 'input_ids': torch.Size([6, 67]), 'token_type_ids': torch.Size([6, 67]), 'labels': torch.Size([6])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a model with the Trainer API"
      ],
      "metadata": {
        "id": "igR6lRrzTnLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer\n",
        "    )\n",
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "1RIRVNLNUCC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")"
      ],
      "metadata": {
        "id": "nCoVoxIgUHqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210,
          "referenced_widgets": [
            "8ae89c11749a4ae7af5d7dd80f0a1ed8",
            "ce226a3b1b914a25b432d8d0240baabb",
            "ba03293bd5fa4d38888947fc764f1d7b",
            "21420de77ce54c169139201b99ceac34",
            "385c5f3be32546c8abe5faa308a4c33c",
            "a35e1f0e5b3d4348bf96a866b3be5d79",
            "26a6233e6f9246a7be33599fe34f608c",
            "0f8d6ee8a0af4f55aa19dcfde12360ab",
            "20d9a2a3766a4d07ad746dd560eca686",
            "efe120f19cdf4b23b93ffe1a557f11ba",
            "f29d04ae86b049328e3f378fa8bdfbd4"
          ]
        },
        "id": "lGK3N3rjW8Nd",
        "outputId": "e57d234a-5d46-411d-c060-ca2f7d255e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ae89c11749a4ae7af5d7dd80f0a1ed8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(valid_preds):\n",
        "  metric = load_metric(\"glue\", \"mrpc\"),\n",
        "  logits, labels = valid_preds\n",
        "  preds_cls = np.argmax(logits, axis=-1)\n",
        "  return metric.compute(predictions=preds_cls, references=labels)"
      ],
      "metadata": {
        "id": "Rfe_l8OuY_1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "SDo-aL4qW8di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "nxn_GahEWsHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A full training"
      ],
      "metadata": {
        "id": "8aceUinXZte1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AdamW,\n",
        "    get_scheduler\n",
        ")\n",
        "from datasets import load_metric\n",
        "# from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "ryoUw93sU-rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = tokenized_ds.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
        "tokenized_ds.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "YZNpDcpUZugU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(\n",
        "    tokenized_ds[\"train\"], \n",
        "    shuffle=True, \n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "valid_dl = DataLoader(\n",
        "    tokenized_ds[\"validation\"], \n",
        "    batch_size=8, \n",
        "    collate_fn=data_collator\n",
        "    )"
      ],
      "metadata": {
        "id": "szBUPwdtU9HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dl:  # padding is applied to max length of respective batch\n",
        "  print({k: v.shape for k, v in batch.items()})\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U09jPUQkVXji",
        "outputId": "dec6b611-db3b-465f-f0f0-f4bc206e129a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': torch.Size([8, 72]), 'input_ids': torch.Size([8, 72]), 'labels': torch.Size([8]), 'token_type_ids': torch.Size([8, 72])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "lESM-2gRVpIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "BLXvuV8aWKZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accelerator = Accelerator()"
      ],
      "metadata": {
        "id": "q8nZZAljZfkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dl)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "sSo74MQLWxun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# train_dl, valid_dl, model, optimizer = accelerator.prepare(\n",
        "#    train_dl, \n",
        "#    valid_dl,\n",
        "#    model,\n",
        "#    optimizer\n",
        "#)  # --> handles device placement so putting model and data on device during training is unnecessary when working with Accelerator"
      ],
      "metadata": {
        "id": "b6KbMVcsXAIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for batch in train_dl:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "  \n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    # accelerator.backward(loss)\n",
        "    \n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    progress_bar.update(1)"
      ],
      "metadata": {
        "id": "4ly_e_-oXUD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"glue\", \"mrpc\")\n",
        "model.eval()\n",
        "for batch in valid_dl:\n",
        "  batch = {k: v.to(device) for k, v in batch.items()}\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**batch)\n",
        "  logits = outputs.logits\n",
        "  predictions = torch.argmax(logits, dim=-1)\n",
        "  metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "metric.compute()"
      ],
      "metadata": {
        "id": "lsXCBYjdYCBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Datasets library"
      ],
      "metadata": {
        "id": "D8nETDBoRMLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import (\n",
        "    load_dataset_builder, \n",
        "    load_dataset,\n",
        ")\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import(\n",
        "    DataLoader\n",
        ")"
      ],
      "metadata": {
        "id": "6lVynTbIRDi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_builder = load_dataset_builder(path=\"poem_sentiment\")\n",
        "\n",
        "train_dataset = load_dataset(path=\"poem_sentiment\", split=\"train\")\n",
        "# valid_dataset = load_dataset(path=\"poem_sentiment\", split=\"validation\")\n",
        "# test_dataset = load_dataset(path=\"poem_sentiment\", split=\"test\")"
      ],
      "metadata": {
        "id": "q1_6R2RdRabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Description:\", train_dataset.description)\n",
        "print(\"Num data entries:\", len(train_dataset))\n",
        "print(\"Column names:\", train_dataset.column_names)\n",
        "print(\"Classes:\", train_dataset.features[\"label\"].names)\n",
        "print(\"Example data entry:\", train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYEbNPw0ThoP",
        "outputId": "ceb3e084-91ef-488b-e0c6-3e13b6670f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description: Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg. This dataset can be used for tasks such as sentiment classification or style transfer for poems.\n",
            "\n",
            "Num data entries: 892\n",
            "Column names: ['id', 'verse_text', 'label']\n",
            "Classes: ['negative', 'positive', 'no_impact', 'mixed']\n",
            "Example data entry: {'id': 0, 'verse_text': 'with pale blue berries. in these peaceful shades--', 'label': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "bKBHTgjCUS6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc_ds = train_dataset.map(lambda examples: \n",
        "                                          tokenizer(\n",
        "                                              examples[\"verse_text\"], \n",
        "                                              truncation=True,\n",
        "                                              padding=\"max_length\",\n",
        "                                          ),\n",
        "                                 batched=True\n",
        "                                 )"
      ],
      "metadata": {
        "id": "C_szpKZAVZws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Column names of encoded dataset:\", train_enc_ds.column_names)\n",
        "print(\"Tokenized data entry:\", train_enc_ds[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfoaHcQJXmUi",
        "outputId": "53f4e7a5-bfd8-4111-aa3d-2e58e135195b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names of encoded dataset: ['attention_mask', 'id', 'input_ids', 'label', 'token_type_ids', 'verse_text']\n",
            "Tokenized data entry: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 0, 'input_ids': [101, 1114, 4554, 2221, 26571, 119, 1107, 1292, 9441, 16327, 118, 118, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'verse_text': 'with pale blue berries. in these peaceful shades--'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use dataset with pytorch\n",
        "train_enc_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# create pytorch data loader\n",
        "train_dl = DataLoader(train_enc_ds, batch_size=32)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2qNF-9iZmYy",
        "outputId": "aa26e791-b87f-42d0-8cfb-2b77679b5f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'input_ids': tensor([[  101,  1114,  4554,  ...,     0,     0,     0],\n",
              "         [  101,  1122,  5611,  ...,     0,     0,     0],\n",
              "         [  101,  1105,  1115,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  1106,  1115,  ...,     0,     0,     0],\n",
              "         [  101,   192,  2386,  ...,     0,     0,     0],\n",
              "         [  101,  1123, 15219,  ...,     0,     0,     0]]),\n",
              " 'label': tensor([1, 2, 0, 3, 3, 3, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 2, 2, 1,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Tokenizers library"
      ],
      "metadata": {
        "id": "z-a0M-C9bcwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training a model from scratch, it also makes sense to train the tokenizer from scratch. (Training a model from scratch makes sense if a model is not available in a particular language or if your corpus is very different from those other models were trained on). So whenever you want to pretrain a model and the dataset is different from the on used by an existing pretraiend model, you have to train a new tokenizer."
      ],
      "metadata": {
        "id": "RGJTWieacTyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a new tokenizer from an old one"
      ],
      "metadata": {
        "id": "GUd0ekDEceeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenizer needs to be trained on a corpus in order to identify subwords that are of interest and occur most frequently in the corpus. Training a tokenizer is unlike training a model - it's a statistical process with the exact rules depending on the tokenization algorithm. It's not random (like model) but deterministic."
      ],
      "metadata": {
        "id": "jfpxfNMpc8VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "mJP_o_Z4bcFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ds = load_dataset(\"code_search_net\", \"python\")"
      ],
      "metadata": {
        "id": "jrUVMzDpdyxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_ds[\"train\"])\n",
        "print(raw_ds[\"train\"][1][\"whole_func_string\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvErSNFgd2XN",
        "outputId": "5e81937d-c2d7-44bc-eb88-18a382a15c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
            "    num_rows: 412178\n",
            "})\n",
            "def close(self):\n",
            "        '''\n",
            "        Cleanly shutdown the router socket\n",
            "        '''\n",
            "        if self._closing:\n",
            "            return\n",
            "        log.info('MWorkerQueue under PID %s is closing', os.getpid())\n",
            "        self._closing = True\n",
            "        # pylint: disable=E0203\n",
            "        if getattr(self, '_monitor', None) is not None:\n",
            "            self._monitor.stop()\n",
            "            self._monitor = None\n",
            "        if getattr(self, '_w_monitor', None) is not None:\n",
            "            self._w_monitor.stop()\n",
            "            self._w_monitor = None\n",
            "        if hasattr(self, 'clients') and self.clients.closed is False:\n",
            "            self.clients.close()\n",
            "        if hasattr(self, 'workers') and self.workers.closed is False:\n",
            "            self.workers.close()\n",
            "        if hasattr(self, 'stream'):\n",
            "            self.stream.close()\n",
            "        if hasattr(self, '_socket') and self._socket.closed is False:\n",
            "            self._socket.close()\n",
            "        if hasattr(self, 'context') and self.context.closed is False:\n",
            "            self.context.term()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It makes sense converting the dataset to an iterator (e.g., a list of lists of texts), so that the tokenizer can train on batches of texts AND the whole dataset does not need to be loaded into memory all at once."
      ],
      "metadata": {
        "id": "niJzdD6sekZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training_corpus = [\n",
        "#                   raw_ds[\"train\"][i: i + 1000][\"whole_func_string\"] \n",
        "#                   for i in range(0, len(raw_ds[\"train\"]), 1000)\n",
        "#                   ]\n",
        "# creates a list of lists of 1000 texts each and load it into memory\n",
        "\n",
        "# use parentheses instead of brackets to create a Python generator that does not\n",
        "# load anything into memory until it's necessary.\n",
        "# nothing is loaded into memory but object is created that can be used in Python\n",
        "# for loop - however, the object can only be used once\n",
        "\n",
        "training_corpus = (\n",
        "    raw_ds[\"train\"][i: i + 1000][\"whole_func_string\"] \n",
        "    for i in range(0, len(raw_ds[\"train\"]), 1000)\n",
        ")"
      ],
      "metadata": {
        "id": "U-n5lCyIeTed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen1 = (i for i in range(10))\n",
        "gen2 = [i for i in range(10)]\n",
        "print(gen1)\n",
        "print(gen2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvPQ_SBzf_kt",
        "outputId": "44cf973b-af34-49ea-d410-ae7bb70ac0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object <genexpr> at 0x7f8cf2e11b50>\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "  return (\n",
        "      raw_ds[\"train\"][i: i + 1000][\"whole_func_string\"]\n",
        "      for i in range(0, len(raw_ds[\"train\"]), 1000)\n",
        "  )\n",
        "\n",
        "# ALTERNATIVE: use yield statement to create generator\n",
        "# def get_training_corpus():\n",
        "#  for start_idx in range(0, len(raw_ds[\"train\"]), 1000):\n",
        "#    yield raw_ds[\"train\"][start_idx: start_idx + 1000][\"whole_func_string\"] \n",
        "\n",
        "training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "FBr2lOekgSep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# not starting *entirely* from scratch but using tokenization algorithm of GPT-2"
      ],
      "metadata": {
        "id": "3P0400jJhEbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "print(old_tokenizer.tokenize(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhgpSlrzh2q7",
        "outputId": "7f0cb140-c491-42a2-bd23-ccc26766565d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, vocab_size=52000)"
      ],
      "metadata": {
        "id": "UUcuKWIpiNgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxu1FWWyihYO",
        "outputId": "c3fa25a7-6ae1-4e8f-e7cd-ed1ef2bb1faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization and pre-tokenization"
      ],
      "metadata": {
        "id": "zgoXeuB_n2A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before splitting a text into subtokens, a tokenizer performs normalization and pre-tokenization before.\n",
        "\n",
        "**normalization** = general cleaning e.g., removing unnecessary whitespaces, lowercasing, removing accents"
      ],
      "metadata": {
        "id": "Q_lP-_LHoB3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "VKqwdfgikES2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "Bw6PJ1cgoWWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Hellö, how ARE YOU tòday?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYoNhxphoinn",
        "outputId": "69b4f1e6-f269-44e4-ccbe-937b00762405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello, how are you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pre-tokenization** = splitting the text into smaller entities i.e., words (e.g., at whitespaces or punctuation) (which are later split into tokens)."
      ],
      "metadata": {
        "id": "3YxeoKe9pW2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"hello, how are you today?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGx-_MW6o74h",
        "outputId": "471073e6-d53a-40e4-d2ca-19be106fbc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (15, 18)), ('today', (19, 24)), ('?', (24, 25))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Byte-Pair Encoding (BPE) tokenization"
      ],
      "metadata": {
        "id": "LmHV_SVrp9-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- used for training GPT models\n",
        "\n",
        "Training of tokenizer:\n",
        "- first, normalization and pre-tokenization are applied\n",
        "- computes unique set of words and builds vocabulary by taking all the symbols used to write those words\n",
        "  - corpus: `[\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]`\n",
        "  - initial vocabulary: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]` \n",
        "- vocabulary is expanded by learning merges i.e., merging two elements of existing vocabulary - as such, longer and longer subwords are added to the vocabulary\n",
        "  - this is done by looking by the most frequent \"pair\" i.e., the most frequently appearing merge between elements in the existing vocabulary. The most frequent pair will be merged and added to the vocabulary. Then the processed is repeated.\n",
        "  - vocabulary during merging: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]` --> `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]` --> `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]` --> ...\n",
        "\n",
        "Tokenization with trained tokenizer:\n",
        "- normalization\n",
        "- pre-tokenization\n",
        "- splitting words into individual characters\n",
        "- applying merge rules learned in order to those splits\n",
        "- example: `\"unhug\"` will be tokenized as `[\"un\", \"hug\"]`, `\"bug\"` as `[\"[UNK]\", \"ug\"]` (since \"b\" was not in the corpus with which tokenizer was trained)"
      ],
      "metadata": {
        "id": "HEb2WBXPq5Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordPiece tokenization"
      ],
      "metadata": {
        "id": "E8XZ1qogiy8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- starts like BPE from base vocabulary and then learns merging rules\n",
        "- does not select most frequent pairs (like BPE) but calculates special score for each pair\n",
        "- does not save the rules learned from training (like BPE) but saves only the final vocabulary"
      ],
      "metadata": {
        "id": "9N_lycAgi2g1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigram tokenization"
      ],
      "metadata": {
        "id": "Vjq2PFevjfNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- starts from a big vocabulary and removes tokens from it until it reaches a desired vocabulary size\n",
        "computes loss over the corpus given the current vocabulary and iterativels removes the 10-20% of tokens based on how little the loss would increase by their removal\n",
        "\n"
      ],
      "metadata": {
        "id": "968REWbvji7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a tokenizer, block by block"
      ],
      "metadata": {
        "id": "BWPnqZxLkjYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization consists of \n",
        "- normalization\n",
        "- pre-tokenization\n",
        "- tokenizer model (like BPE, WordPiece, ...)\n",
        "- post-processing (adding special tokens, generating attention mask and token type IDs)"
      ],
      "metadata": {
        "id": "ygEYdRDTkxLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer\n",
        ")\n",
        "from transformers import (\n",
        "    PreTrainedTokenizerFast,\n",
        "    BertTokenizerFast\n",
        ")"
      ],
      "metadata": {
        "id": "WrI02LMCkoo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
      ],
      "metadata": {
        "id": "U5vO4PVBqX-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "  for i in range(0, len(ds), 1000):\n",
        "    yield ds[i: i + 1000][\"text\"]"
      ],
      "metadata": {
        "id": "hyTNX8zCqgpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model block\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "FuoWC5uaseKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizer block \n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [\n",
        "     normalizers.NFD(),  # NFD Unicode normalizer\n",
        "     normalizers.Lowercase(),\n",
        "     normalizers.StripAccents()\n",
        "    ]\n",
        ")\n",
        "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFUlMEs4sjKk",
        "outputId": "5ccd85f5-73bd-4c99-bd6f-ed23713c7cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are u?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-tokenizer block\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace() # splits of whitespaces and punctuation (excluding underscore character)\n",
        "print(tokenizer.pre_tokenizer.pre_tokenize_str(\"Today's such a beautiful and sunny day.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FPSlIzPtLXM",
        "outputId": "07dff018-b2e9-4c8c-c8d3-758c97610add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Today', (0, 5)), (\"'\", (5, 6)), ('s', (6, 7)), ('such', (8, 12)), ('a', (13, 14)), ('beautiful', (15, 24)), ('and', (25, 28)), ('sunny', (29, 34)), ('day', (35, 38)), ('.', (38, 39))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train tokenizer\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "Y-jJfk6Hts2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test the tokenizer, shall we?\")\n",
        "print(\"Tokens:\", encoding.tokens)\n",
        "print(\"Input IDs:\", encoding.ids)\n",
        "print(\"Offsets:\", encoding.offsets)\n",
        "print(\"Attention mask:\", encoding.attention_mask)\n",
        "print(\"Special token mask:\", encoding.special_tokens_mask)\n",
        "print(\"Overflowing:\", encoding.overflowing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ati3a0duKqq",
        "outputId": "6b6c6087-3e5f-4754-c3b9-a2791bc7da9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['let', \"'\", 's', 'test', 'the', 'tok', '##eni', '##zer', ',', 'shall', 'we', '?']\n",
            "Input IDs: [3019, 11, 61, 3611, 1333, 24319, 18903, 6614, 16, 11448, 1626, 35]\n",
            "Offsets: [(0, 3), (3, 4), (4, 5), (6, 10), (11, 14), (15, 18), (18, 21), (21, 24), (24, 25), (26, 31), (32, 34), (34, 35)]\n",
            "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Special token mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Overflowing: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# post-processing\n",
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\") # to add at beginning of sequence\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\") # to add at end of each sentence in sequence\n",
        "print(\"[CLS]\", cls_token_id, \"[SEP]\", sep_token_id)\n",
        "\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS]:0 $A:0 [SEP]:0\", # token type IDs for single sentences ($A means single sentence)\n",
        "    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\", # tokent type IDs to use for sentence pairs ($B)\n",
        "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztN3IveHupZ-",
        "outputId": "8b7b7a0e-f0c9-48d1-c398-6362b2ee884e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] 2 [SEP] 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test the tokenizer, shall we?\")\n",
        "print(\"Tokens:\", encoding.tokens)\n",
        "print(\"Input IDs:\", encoding.ids)\n",
        "print(\"Offsets:\", encoding.offsets)\n",
        "print(\"Attention mask:\", encoding.attention_mask)\n",
        "print(\"Special token mask:\", encoding.special_tokens_mask)\n",
        "print(\"Overflowing:\", encoding.overflowing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsYX7Dddvizu",
        "outputId": "9611ae75-43ab-4009-865a-323180b08c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['[CLS]', 'let', \"'\", 's', 'test', 'the', 'tok', '##eni', '##zer', ',', 'shall', 'we', '?', '[SEP]']\n",
            "Input IDs: [2, 3019, 11, 61, 3611, 1333, 24319, 18903, 6614, 16, 11448, 1626, 35, 3]\n",
            "Offsets: [(0, 0), (0, 3), (3, 4), (4, 5), (6, 10), (11, 14), (15, 18), (18, 21), (21, 24), (24, 25), (26, 31), (32, 34), (34, 35), (0, 0)]\n",
            "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Special token mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "Overflowing: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder\n",
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
        "\n",
        "tokenizer.decode(encoding.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "r8sAmVB3wyvx",
        "outputId": "7eadf2df-a7a9-4d82-bcfe-4759938d7482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"let's test the tokenizer, shall we?\""
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokenizer\n",
        "tokenizer.save(\"tokenizer.json\")\n",
        "\n",
        "#load tokenizer\n",
        "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "A_v83ZDZxQa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap tokenizer in PreTrainedTokenizerFast\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    # tokenizer_file=\"tokenizer.json\",  # alternatively, load from file \n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\"\n",
        ")"
      ],
      "metadata": {
        "id": "BJqTbbhKxcV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a BPE (GPT-2) tokenizer from scratch:"
      ],
      "metadata": {
        "id": "gRVo5fI-y6R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose model\n",
        "tokenizer = Tokenizer(models.BPE())"
      ],
      "metadata": {
        "id": "Um1EU0yfy16l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add pre-tokenizer (not normalizer needed for GPT-2 tokenizer)\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) # add_prefix_space add space at beginning of sentence"
      ],
      "metadata": {
        "id": "t58tA0vjy_gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train tokenizer\n",
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "O5AjMiDezjr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post-processing\n",
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False) # trim_offset means that start of offset will point at first character of word, not at space before it\n"
      ],
      "metadata": {
        "id": "PPVJakumz5UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder\n",
        "tokenizer.decoder = decoders.ByteLevel()"
      ],
      "metadata": {
        "id": "gVrEuUAW0hSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\"\n",
        ")\n",
        "# OR\n",
        "# wrapped_tokenizer = GPT2TokenizerFast(tokenier_object=tokenizer)"
      ],
      "metadata": {
        "id": "r7jXofmR0m52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Main NLP tasks"
      ],
      "metadata": {
        "id": "M1nxM2Hf2daM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token classification\n",
        "\n",
        "- **Named entity recognition (NER)**: find names, locations and organization in a sentence; assign class to each token\n",
        "- **Part-of-speech tagging (POS)**: mark each word as its respective par of speech (e.g., noun, verb,...)\n",
        "- **Chunking**: find tokens that belong to the same entity"
      ],
      "metadata": {
        "id": "XXGQZvCq30ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a masked language model\n",
        "\n",
        "- pre-trained model can be fine-tuned on your data if the dataset is not too different from the corpus used from pretraining the model\n",
        "- **domain adaptation**: on example of fine-tuning a model, namely on a dataset of special domain texts"
      ],
      "metadata": {
        "id": "eXQi92am52Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "vfn6juwu5hMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "YcOTQhBX6o-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how pretrained model works \n",
        "text = \"This is a great [MASK].\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "token_logits = model(**inputs).logits\n",
        "mask_token_idx = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "mask_token_logits = token_logits[0, mask_token_idx, :]\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "  print(f\"{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNYj2YxxAX2d",
        "outputId": "530ce426-b632-4ff5-811f-34ddfd1f273f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a great deal.\n",
            "This is a great success.\n",
            "This is a great adventure.\n",
            "This is a great idea.\n",
            "This is a great feat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_ds = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "T-bJ9WX6Bb1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imdb_ds[\"train\"][\"text\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pguggYPVC1i1",
        "outputId": "6b8730f1-6ae5-48d6-8f7e-5a0c4eb5c497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_func(examples):\n",
        "  result = tokenizer(examples[\"text\"])\n",
        "  if tokenizer.is_fast:\n",
        "    result[\"word_ids\"] = [result.word_ids(i) \n",
        "    for i in range(len(result[\"input_ids\"]))]\n",
        "  return result"
      ],
      "metadata": {
        "id": "_8hSF93sDDFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = imdb_ds.map(\n",
        "    tokenize_func, \n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"label\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "ze39w25p4XKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhS0UcD-5NR6",
        "outputId": "84264016-b19a-470f-f707-6dee8ed18260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'word_ids'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'word_ids'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'word_ids'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_ds[\"train\"][\"input_ids\"][0])\n",
        "print(tokenized_ds[\"train\"][\"word_ids\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8psrgcx5qgq",
        "outputId": "1ffe7213-ace0-42dc-d7f9-9a0d05a9de74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n",
            "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 216, 217, 218, 218, 219, 220, 221, 222, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 272, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.model_max_length) # max sequence length the model can take\n",
        "\n",
        "tokenized_samples = tokenized_ds[\"train\"][6:9]\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "  print(f\"Review {idx}: length {len(sample)}\")\n",
        "# some examples are longer than max length\n",
        "# solution: concatenate all examples and then split them into chunks of equal size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVLobekO5Drw",
        "outputId": "d741bfde-2d38-4481-a058-9562adba50e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "Review 0: length 143\n",
            "Review 1: length 388\n",
            "Review 2: length 720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the samples \n",
        "concatenated_examples = {\n",
        "    k: [i for sample in tokenized_samples[k] for i in sample] for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(\"Concatenated reviews length:\", total_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9pvKvWt6slF",
        "outputId": "377065df-ebcf-429f-ffd2-ddf39a731680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated reviews length: 1251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the concatenated sample into chunks\n",
        "chunk_size = 128 \n",
        "chunks = {\n",
        "    k: [v[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, v in concatenated_examples.items()\n",
        "} \n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "  print(\"Chunk length:\", len(chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSqaONwa47XZ",
        "outputId": "abdf7f7a-817d-4543-e098-68e9dcf8d493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 128\n",
            "Chunk length: 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap the all of the above code in a function\n",
        "\n",
        "def group_texts(examples):\n",
        "  concatenated_examples = {\n",
        "      k: [i for sample in examples[k] for i in sample] for k in examples.keys()\n",
        "      }\n",
        "  total_length = len(concatenated_examples[\"input_ids\"])\n",
        "  total_length = (total_length // chunk_size) * chunk_size\n",
        "  result = {\n",
        "      k: [v[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "      for k, v in concatenated_examples.items()\n",
        "  }\n",
        "  # labels are the same as input_ids \n",
        "  # (since model will predict randomly masked tokens of input)\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()  \n",
        "  return result"
      ],
      "metadata": {
        "id": "8ttrAScf7YKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_ds = tokenized_ds.map(group_texts, batched=True)"
      ],
      "metadata": {
        "id": "ONODuFuu5k_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fdI5APV-8hy",
        "outputId": "f4099106-c62c-4a83-da17-f9203ef676ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
            "        num_rows: 61291\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
            "        num_rows: 59904\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
            "        num_rows: 122957\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(lm_ds[\"train\"][3][\"input_ids\"]))\n",
        "print(tokenizer.decode(lm_ds[\"train\"][3][\"labels\"]))\n",
        "# labels still need to be randomly masked (done in the data collator)         "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5IMltNB_T-4",
        "outputId": "92497b77-12ea-42f7-8aeb-c2c291ba06f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "' t matter what one's political views are because this film can hardly be taken seriously on any level. as for the claim that frontal male nudity is an automatic nc - 17, that isn't true. i've seen r - rated films with male nudity. granted, they only offer some fleeting views, but where are the r - rated films with gaping vulvas and flapping labia? nowhere, because they don't exist. the same goes for those crappy cable shows : schlongs swinging in the breeze but not a clitoris in sight. and those pretentious indie movies like\n",
            "' t matter what one's political views are because this film can hardly be taken seriously on any level. as for the claim that frontal male nudity is an automatic nc - 17, that isn't true. i've seen r - rated films with male nudity. granted, they only offer some fleeting views, but where are the r - rated films with gaping vulvas and flapping labia? nowhere, because they don't exist. the same goes for those crappy cable shows : schlongs swinging in the breeze but not a clitoris in sight. and those pretentious indie movies like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
        "# mlm_probability = fraction of tokens to mask"
      ],
      "metadata": {
        "id": "1KgMcqdn_4Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data collator expects list of dicts (each dict is one sample)\n",
        "samples = [lm_ds[\"train\"][i] for i in range(2)]\n",
        "for sample in samples:\n",
        "  _ = sample.pop(\"word_ids\")\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "  print(tokenizer.decode(chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrfUzafP_o4W",
        "outputId": "5ec317cf-377e-41e9-9d91-9275a1c17535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] i rented i am hari - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. s. customs ifப ever tried [MASK] [MASK] [MASK] country [MASK] therefore being a fan of films considered \" controversial \" i really had ð see this for [MASK]. [MASK] [MASK] / > < br / > the plot [MASK] centered around a young swedish drama student named lena who wants [MASK] [MASK] everything she can about life. in particular she wants to focus her attentions to making [MASK] [MASK] [MASK] documentary on what the average sw ј thought about certain political issues [MASK]\n",
            "[MASK] the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about [MASK] [MASK] on politics, she has sex with her drama teacher, classmates, and married men. < [MASK] / > < br / > what kills me about i am curious - yellow is that 40 years l, this was considered pornographic. really, the sex and nudity scenes are few and far between [MASK] even then [MASK]'s not shot like some cheaply [MASK] porno. while my [MASK]men mind find it shocking, in reality sex and nu [MASK] are a major staple in swedish [MASK]. even ingmar bergman,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downsample dataset\n",
        "train_size = 10000\n",
        "test_size = int(0.1 * train_size)\n",
        "downsampled_ds = lm_ds[\"train\"].train_test_split(\n",
        "    train_size=train_size,\n",
        "    test_size=test_size,\n",
        "    seed=42\n",
        ")\n",
        "print(downsampled_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DK59WwB_zf4",
        "outputId": "a851633f-3daa-4f51-8951-b211c713b9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
            "        num_rows: 10000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "logging_steps = len(downsampled_ds[\"train\"]) // batch_size\n",
        "model_name = checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    push_to_hub=True,\n",
        "    fp16=True,\n",
        "    logging_steps=logging_steps, # to track training loss each epoch\n",
        ")"
      ],
      "metadata": {
        "id": "HMxnA7kFB6dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "7ma6HEpWzLOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=downsampled_ds[\"train\"],\n",
        "    eval_dataset=downsampled_ds[\"test\"],\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "5hXn8qCyDge-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Pl_9CJhk1PVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "\n",
        "- good model assigns high probability to correct sentences and low probability to incorrect sentences\n",
        "- one way of valutaing model is to look at probability the model assign to the next word in each sentence - if the probabilities are high, it means the model is not \"perplexed\" by the unseen examples --> measuring performance with exponential cross-entropy loss (= perplexity)"
      ],
      "metadata": {
        "id": "00zhXaNH0dZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()"
      ],
      "metadata": {
        "id": "vaR2zqM20IH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation\n",
        "\n",
        "= sequence-to-sequence task\n",
        "\n",
        "Approach of translation can also be adapted to style transfer or generatie question answering.\n",
        "\n",
        "Here, we will fine-tune a Marian model that was pre-trained from English to French."
      ],
      "metadata": {
        "id": "eVXLOTAy1c46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        "    load_metric\n",
        ")\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    AdamW,\n",
        "    get_scheduler\n",
        ")\n",
        "from accelerate import Accelerator\n",
        "from huggingface_hub import (\n",
        "    notebook_login,\n",
        "    Repository,\n",
        "    get_full_repo_name\n",
        "    )\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "dpu2ua-k1nzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "raw_ds = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
      ],
      "metadata": {
        "id": "84LMgOJI2MI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vuOrx4s2Q1l",
        "outputId": "84e799eb-3945-47f3-f80f-8305424f12f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'translation'],\n",
            "        num_rows: 210173\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into train and validation\n",
        "split_ds = raw_ds[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
        "split_ds[\"validation\"] = split_ds.pop(\"test\")"
      ],
      "metadata": {
        "id": "fkF0I9jQ2eGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(split_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT4cNQZC3B7S",
        "outputId": "2be4316e-ed9f-4809-b1a4-e37bbbfed247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'translation'],\n",
            "        num_rows: 189155\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'translation'],\n",
            "        num_rows: 21018\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "print(\"EN:\", split_ds[\"train\"][1][\"translation\"][\"en\"])\n",
        "print(\"FR (dataset):\", split_ds[\"train\"][1][\"translation\"][\"fr\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDkjY6ax3E02",
        "outputId": "299f1a3f-664a-4f9b-ebeb-73c3e3119293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Default to expanded threads\n",
            "FR (dataset): Par défaut, développer les fils de discussion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "translator = pipeline(\"translation\", model=checkpoint)\n",
        "print(\"FR (model output):\", translator(\"Default to expanded threads\")[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWNSYDYq7N5o",
        "outputId": "06acf8dd-41c5-49f6-8439-f7b96ac220c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FR (model output): Par défaut pour les threads élargis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset, all English words are translated to French, whereas the pretrained model was trained on a corpus where some technical English words were used in the French translation - the goal of fine-tuning is to let the model learn the translation of these technical English words"
      ],
      "metadata": {
        "id": "V4hD93-c4zmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "erdMBMdj45vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs are processed normally\n",
        "# tokenizer for targets is wrapped inside context manager with `as_target_tokenizer()`\n",
        "# otherwise target are tokenized by input tokenizer (used for English!!).\n",
        "en_sentence = split_ds[\"train\"][1][\"translation\"][\"en\"]\n",
        "fr_sentence = split_ds[\"train\"][1][\"translation\"][\"fr\"]\n",
        "\n",
        "inputs = tokenizer(en_sentence)\n",
        "incorrect_targets = tokenizer(fr_sentence)\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  targets = tokenizer(fr_sentence)\n",
        "\n",
        "print(\"Incorrect:\", tokenizer.convert_ids_to_tokens(incorrect_targets[\"input_ids\"]))\n",
        "print(\"Correct:\", tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF6sf44S6KBU",
        "outputId": "3035aa96-e285-466b-f181-769cb1c64b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incorrect: ['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n",
            "Correct: ['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_func(examples):\n",
        "  inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "  targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "  # put labels inside inputs dictionary\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "whAvjgI58xoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = split_ds.map(\n",
        "    preprocess_func,\n",
        "    batched=True,\n",
        "    remove_columns=split_ds[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "Ga4THbq299lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "rHru0XfT_W9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data collator with padding for dynamic batching\n",
        "# DataCollatorWithPadding would only pad input, but labels also need to be padded\n",
        "# target should be padded with -100 so that these padding tokens are ignored in loss computation\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "XeJzWWgb_qmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([tokenized_ds[\"train\"][i] for i in range(1, 3)])\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQMqFydAAMax",
        "outputId": "63e8ba4c-1cc8-4525-95dc-ef3f91db4317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[47591,    12,  9842, 19634,     9,     0, 59513, 59513, 59513, 59513,\n",
            "         59513, 59513, 59513, 59513, 59513],\n",
            "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
            "         28149,   139, 33712, 25218,     0]]), 'labels': tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100],\n",
            "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
            "           550,  7032,  5821,  7907, 12649,     0]]), 'decoder_input_ids': tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
            "         59513, 59513, 59513, 59513, 59513, 59513],\n",
            "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
            "           817,   550,  7032,  5821,  7907, 12649]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional metric used in translation: BLEU score (= how close is translation to label?) - uses statistical tools to measure model's output (instead of intelligibility of grammatical correctness)\n",
        "Today, most commonly used score is SacreBLEU (since BLEU expects text already to be tokenized, which is a problem since different models use different tokenizers)."
      ],
      "metadata": {
        "id": "gE0p6ectAiCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "dNiwYeRdAUXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"This plugin lets you translate web pages between several languages automatically.\"]\n",
        "references = [[\"This plugin allows you to automatically translate web pages between several languages.\"]]\n",
        "\n",
        "print(\"SacreBLEU score:\", metric.compute(predictions=predictions, references=references)[\"score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiYRIcjfBYnU",
        "outputId": "cab0e515-9917-49d5-cbb8-357661dbb49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SacreBLEU score: 46.750469682990165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"This This This This\"]\n",
        "references = [[\"This plugin allows you to automatically translate web pages between several languages.\"]]\n",
        "print(\"SacreBLEU score:\", metric.compute(predictions=predictions, references=references)[\"score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFl9QPh5Brql",
        "outputId": "a56f3ed9-b0ba-4b68-d35a-8e8a89d86c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SacreBLEU score: 1.683602693167689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "  preds, labels = eval_preds\n",
        "  # in case model returns more than pred logits\n",
        "  if isinstance(preds, tuple):\n",
        "    preds = preds[0]\n",
        "  # decode preds\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  # decode labels\n",
        "  labels = np.where(labels != -100, labels, zokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  # post-processing\n",
        "  decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "  decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "  result = metric.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "  return {\"blue\": result[\"score\"]}"
      ],
      "metadata": {
        "id": "zmg5I_HhB-JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"marian-finetuned-kde4-en-to-fr\",\n",
        "    evaluation_strategy=\"no\", # evaluate model once before and after training instead\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "DCrG_Ef9DKjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "p68ncRyQDPr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# score before fine-tuning\n",
        "trainer.evaluate(max_length=max_target_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "j1sVHq2DDl2d",
        "outputId": "5b245f0a-7f9c-4ba6-8f50-ae17db1618bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 21018\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/329 20:53 < 45:37, 0.08 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "UoU0VVS1Dv82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate after fine-tuning\n",
        "trainer.evaluate(max_length=max_target_length)"
      ],
      "metadata": {
        "id": "8zbqOQUqD3H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom training loop\n",
        "\n",
        "tokenized_ds.set_format(\"torch\")\n",
        "train_dl = DataLoader(\n",
        "    tokenized_ds[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8\n",
        ")\n",
        "eval_dl = DataLoader(\n",
        "    tokenized_ds[\"validation\"],\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8\n",
        ")"
      ],
      "metadata": {
        "id": "vYj0ir96EFQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "J-k6UbDDEUYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator = Accelerato()\n",
        "model, optimizer, train_dl, eval_dl = accelerator.prepare(\n",
        "    model,\n",
        "    optimizer,\n",
        "    train_dl,\n",
        "    eval_dl\n",
        ")"
      ],
      "metadata": {
        "id": "iTgQWTD9Eb-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs = 3\n",
        "num_update_steps = num train_epochs * len(train_dl) \n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "TRf_BlAME0CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(predictions, labels):\n",
        "  predictions = predictions.cpu().numpy()\n",
        "  labels = labels.cpu().numpy()\n",
        "\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  \n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  \n",
        "  decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "  decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "  \n",
        "  return decoded_preds, decoded_labels"
      ],
      "metadata": {
        "id": "pF2qRQVNFALd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_training_epochs):\n",
        "  # training\n",
        "  model.train()\n",
        "  for batch in train_dl:\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    progress_bar.update(1)\n",
        "\n",
        "  # evaluation\n",
        "  model.eval()\n",
        "  for batch in tqdm(eval_dl):\n",
        "    with torch.no_grad():\n",
        "      generated_tokens = accelerator.unwrap_model(model).generate(\n",
        "          batch[\"input_ids\"],\n",
        "          attention_mask=batch[\"attention_mask\"],\n",
        "          max_langth=128\n",
        "      )\n",
        "    labels = batch[\"labels\"]\n",
        "\n",
        "    generated_tokens = accelerator.pad_across_processes(\n",
        "            generated_tokens, \n",
        "            dim=1, \n",
        "            pad_index=tokenizer.pad_token_id\n",
        "        )\n",
        "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
        "\n",
        "    predictions_gathered = accelerator.gather(generated_tokens)\n",
        "    labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "    decoded_preds, decoded_labels = prostprocess(\n",
        "        predictions_gathered, \n",
        "        labels_gathered\n",
        "        )\n",
        "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "  results = metric.compute()\n",
        "  print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")"
      ],
      "metadata": {
        "id": "eZoXt_clFYwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}