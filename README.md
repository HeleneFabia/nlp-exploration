<p align="center">
  <img width="600" height="180" src="https://github.com/HeleneFabia/nlp-exploration/blob/main/images/nlp.png">
</p>

I've always been fascinated by language. I studied language at university (translation and language teaching) and love to practice my foreign language skills when aborad. My curiosity for languages is also what brought me into the realms of computer languages (and hence, to my current job as an ML engineer). 

I currently work in the field of Computer Vision - however, due to my language affinity, I have also been always drawn to NLP. Since I spent most of my time last year working with images and CNNs, I wanted to dedicate the bgeinning of this year to re-freshing my NLP knowledge and dabbling into HuggingFace, SpaCy and nltk. 

## HuggingFace

[See my notebook here](https://github.com/HeleneFabia/nlp-exploration/blob/main/notebooks/nlp_with_huggingface.ipynb)

What I learned: 
- pretraining and fine-tuning a language model with HuggingFace's Trainer API as well as with a custom trainer loop) (e.g. a translation model)
- evaluating a language model (e.g. using the BLEU score to evaluate a translation model)
- basic buildings blocks of transformer models
- basic workings of a tokenizer
  - normalization
  - pre-tokenization
  - tokenization-algorithms (e.g. BPE, WordPiece, Unigram)
  - decoding
  - padding and dynamic padding
  - truncation
  - special tokens
- pretraining and fine-tuning a tokenizer 
- types of model inputs (input IDs, token type IDs, attention masks, word IDs)
- running inference on HuggingFace's OTB models
- using HuggingFace's datasets library
